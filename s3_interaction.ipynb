{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21d9e590",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# ðŸª£ Working with Amazon S3 Object Storage\n",
    "\n",
    "This lab demonstrates how to work with **Amazon S3 (Simple Storage Service)** using both the **AWS Management Console** and programmatically via the **Boto3 SDK**.  \n",
    "You will create an S3 bucket, upload and query structured and semi-structured data, configure permissions and versioning, and automate tasks with reusable Python functions.\n",
    "You can find all the code examples about interaction with S3 in Boto3 [Documentations](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-examples.html)\n",
    "\n",
    "By completing this lab, you will be able to:\n",
    "\n",
    "- Create and configure an **S3 bucket** using Terraform or the AWS console.  \n",
    "- Upload **structured (CSV)**, **semi-structured (JSON)**, and **unstructured (image)** data to Amazon S3.  \n",
    "- Interact with S3 objects programmatically using **Boto3**.  \n",
    "- Configure **bucket access policies** and **public access settings**.  \n",
    "- Enable and inspect **versioning** in S3.  \n",
    "- Clean up and **delete buckets** and objects, including versioned ones.\n",
    "\n",
    "The required data are located in **data** folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adab515",
   "metadata": {},
   "source": [
    "## 1. Insttall the required Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1124a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install boto3\n",
    "%pip install python-dotenv boto3\n",
    "%pip install -q pandas s3fs python-dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57c0dfe",
   "metadata": {},
   "source": [
    "# 1. Import the required liberaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c83b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import boto3\n",
    "import mimetypes\n",
    "\n",
    "from typing import Any, Dict\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0024f52",
   "metadata": {},
   "source": [
    "## âš™ï¸ 1. Setting Up AWS Credentials\n",
    "\n",
    "Before running any S3 operations, configure your AWS credentials securely.  \n",
    "We store credentials in a `.env` file and create a **global session** that all Boto3 clients will use automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6350032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Global AWS session configured for us-west-2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Force .env to override anything already set in your terminal\n",
    "\n",
    "# Load and clean .env\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "def clean(v): \n",
    "    return v.strip().strip('\"').strip(\"'\") if v else v\n",
    "\n",
    "AWS_ACCESS_KEY_ID     = clean(os.getenv(\"AWS_ACCESS_KEY_ID\"))\n",
    "AWS_SECRET_ACCESS_KEY = clean(os.getenv(\"AWS_SECRET_ACCESS_KEY\"))\n",
    "AWS_DEFAULT_REGION    = clean(os.getenv(\"AWS_DEFAULT_REGION\")) or \"us-west-2\"\n",
    "\n",
    "# Create a global session (used automatically by all boto3 clients)\n",
    "boto3.setup_default_session(\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    region_name=AWS_DEFAULT_REGION,\n",
    ")\n",
    "\n",
    "print(\" Global AWS session configured for\", AWS_DEFAULT_REGION)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ca46f6",
   "metadata": {},
   "source": [
    "# 4. Uploading Files to S3\n",
    "\n",
    "\n",
    "The following reusable function uploads any file from your local environment to an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7338cf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def upload_file_to_s3(local_file_path: str, bucket_name: str, object_key: str) -> None:\n",
    "    \"\"\"\n",
    "    Uploads a local file to S3 using boto3 with automatic content type detection.\n",
    "\n",
    "    Args:\n",
    "        local_file_path (str): Path to the local file to upload.\n",
    "        bucket_name (str): Name of the target S3 bucket.\n",
    "        object_key (str): S3 object key (path/name in the bucket).\n",
    "    \"\"\"\n",
    "    local_file = Path(local_file_path)\n",
    "    if not local_file.exists():\n",
    "        raise FileNotFoundError(f\"Local file not found: {local_file_path}\")\n",
    "\n",
    "    # Detect MIME type for correct Content-Type metadata\n",
    "    ctype, _ = mimetypes.guess_type(local_file.name)\n",
    "    extra = {\"ContentType\": ctype} if ctype else {}\n",
    "\n",
    "    # Build the S3 client\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "\n",
    "    try:\n",
    "        s3_client.upload_file(str(local_file), bucket_name, object_key, ExtraArgs=extra)\n",
    "        print(f\"Uploaded {local_file} â†’ s3://{bucket_name}/{object_key}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading file to S3: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a2af0",
   "metadata": {},
   "source": [
    "# a. Upload the Structured File\n",
    "Upload your csv file into the bucket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f268a4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded data\\csv\\ratings_ml_training_dataset.csv â†’ s3://example-bucket-d2e948dbd0db/csv/ratings_ml_training_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the local file path, and object key\n",
    "local_file_path = 'data/csv/ratings_ml_training_dataset.csv'\n",
    "object_key = 'csv/ratings_ml_training_dataset.csv'\n",
    "BUCKET_NAME='example-bucket-d2e948dbd0db'\n",
    "\n",
    "# Upload the file to S3\n",
    "upload_file_to_s3(local_file_path, BUCKET_NAME, object_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcee156",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check the uploaded file in the bucket\n",
    "\n",
    "!aws s3 ls $BUCKET_NAME/csv/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044e2768",
   "metadata": {},
   "source": [
    "## Query the the uploaded csv\n",
    "You can read structured data directly from S3 into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bceeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "import os, pandas as pd\n",
    "\n",
    "BUCKET = \"example-bucket-d2e948dbd0db\"   \n",
    "KEY    = \"csv/ratings_ml_training_dataset.csv\"\n",
    "\n",
    "s3_path = f\"s3://{BUCKET}/{KEY}\"\n",
    "df = pd.read_csv(s3_path)\n",
    "\n",
    "# optional: tidy column names\n",
    "df.columns = [c.strip().lower() for c in df.columns]\n",
    "print(\"Rows:\", len(df), \"Cols:\", len(df.columns))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec7d92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: rows for France with quantityinstock > 5000, select a few columns\n",
    "q1 = (\n",
    "    df.loc[(df[\"country\"] == \"France\") & (df[\"quantityinstock\"] > 5000),\n",
    "           [\"city\", \"country\", \"productcode\", \"productline\", \"quantityinstock\", \"buyprice\"]]\n",
    "    .sort_values(\"quantityinstock\", ascending=False)\n",
    ")\n",
    "q1.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa25c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) stock by country\n",
    "stock_by_country = (\n",
    "    df.groupby(\"country\", as_index=False)[\"quantityinstock\"].sum()\n",
    "    .sort_values(\"quantityinstock\", ascending=False)\n",
    ")\n",
    "\n",
    "# 2) average price by product line\n",
    "avg_prices = (\n",
    "    df.groupby(\"productline\", as_index=False)\n",
    "      .agg(avg_buyprice=(\"buyprice\", \"mean\"), avg_msrp=(\"msrp\", \"mean\"))\n",
    "      .sort_values(\"avg_buyprice\", ascending=False)\n",
    ")\n",
    "\n",
    "# show results\n",
    "display(stock_by_country.head(10))\n",
    "display(avg_prices.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0c0670",
   "metadata": {},
   "source": [
    "## b. Upload the semi-structured file using the pere-built function\n",
    "Now upload semi-structured data (a JSON file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ad3a1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded data\\json\\delivery-stream-one-record.json â†’ s3://example-bucket-d2e948dbd0db/json/delivery-stream-one-record.json\n"
     ]
    }
   ],
   "source": [
    "# Define the local file path, and object key\n",
    "local_file_path = 'data/json/delivery-stream-one-record.json'\n",
    "object_key = 'json/delivery-stream-one-record.json'\n",
    "BUCKET_NAME='example-bucket-d2e948dbd0db'\n",
    "\n",
    "# Upload the file to S3\n",
    "upload_file_to_s3(local_file_path, BUCKET_NAME, object_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60034feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $BUCKET_NAME/json/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8a52bd",
   "metadata": {},
   "source": [
    "## 5. Downloading Files from S3\n",
    "You can download files locally with another reusable function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bea286",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def download_file_from_s3(bucket_name: str, object_key: str, local_file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Downloads an object from S3 using the existing authenticated `s3` client.\n",
    "    \"\"\"\n",
    "    dest = Path(local_file_path)\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        s3_client.download_file(bucket_name, object_key, str(dest))\n",
    "        print(f\"Downloaded s3://{bucket_name}/{object_key} â†’ {dest}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading from S3: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a8f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define local destination paths for csv file\n",
    "csv_local = \"./downloads/ratings_ml_training_dataset.csv\"\n",
    "json_local = \"./downloads/delivery-stream-one-record.json\"\n",
    "BUCKET='example-bucket-d2e948dbd0db'\n",
    "\n",
    "download_file_from_s3(BUCKET, json_key, json_local)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e7b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define local destination paths for json file\n",
    "csv_key = \"csv/ratings_ml_training_dataset.csv\"\n",
    "json_key = \"json/delivery-stream-one-record.json\"\n",
    "BUCKET_NAME='example-bucket-d2e948dbd0db'\n",
    "\n",
    "\n",
    "download_file_from_s3(BUCKET_NAME, csv_key, csv_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1c4928",
   "metadata": {},
   "source": [
    "# c. Upload the Unstructured file \n",
    "This functions uploads the image file into the s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1859b277",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_file_path_image_v1 = 'data/images/v1/s3_logo.png'\n",
    "object_key_image = 'images/s3_logo.png'\n",
    "\n",
    "upload_file_to_s3(local_file_path_image_v1, BUCKET_NAME, object_key_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ecc2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the folder in the bucket\n",
    "!aws s3 ls $BUCKET_NAME/images/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f974ca",
   "metadata": {},
   "source": [
    "Now you can also check the uploaded folders and files in the bucket in AWS Console\n",
    "\n",
    "![S3 Architecture](assets/img2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88847d7",
   "metadata": {},
   "source": [
    "## 6. Managing Bucket Access Settings\n",
    "\n",
    "Amazon S3 buckets are **private by default**, meaning that only the AWS account owner and explicitly authorized users or roles can access them.  \n",
    "To securely share data through web browser or make certain objects public (for example, images or static web files), you must configure **bucket access settings** properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b37ac63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_public_access_setup(bucket_name: str, public_access_block_configuration: Dict[str, Any]) -> None:\n",
    "    \"\"\"Sets public access configuration for S3 bucket\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): Bucket name\n",
    "        public_access_block_configuration (Dict[str, Any]): Configuration for public access\n",
    "    \"\"\"\n",
    "    \n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    \n",
    "    # Update the bucket's public access settings\n",
    "    s3_client.put_public_access_block(\n",
    "        Bucket=bucket_name,\n",
    "        PublicAccessBlockConfiguration=public_access_block_configuration\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0655991d",
   "metadata": {},
   "source": [
    "### ðŸ§­ Understanding Public Access Configuration\n",
    "\n",
    "S3 provides a set of **public access block settings** that act as a safety layer, preventing accidental exposure of sensitive data. These settings can be enabled or disabled at the bucket level.\n",
    "\n",
    " **Caution**: Disabling public access protection makes your bucket publicly readable. Use only in a safe, temporary test environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0998fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the public access settings  \n",
    "public_access_configuration = {\n",
    "    'BlockPublicAcls': False,\n",
    "    'IgnorePublicAcls': False,\n",
    "    'BlockPublicPolicy': False,\n",
    "    'RestrictPublicBuckets': False\n",
    "}\n",
    "\n",
    "s3_public_access_setup(bucket_name=BUCKET_NAME, \n",
    "                       public_access_block_configuration=public_access_configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70988c1",
   "metadata": {},
   "source": [
    "## 7. Setting a Bucket Policy\n",
    "Define a policy that controls permissions for specific prefixes or users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bf661a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_put_bucket_policy(bucket_name: str, policy: Dict[str, Any]) -> None:\n",
    "    \"\"\"Allows to put bucket policies\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): Bucket name\n",
    "        policy (Dict[str, Any]): Bucket policy\n",
    "    \"\"\"\n",
    "    \n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    \n",
    "    response = s3_client.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(policy))\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb8c963",
   "metadata": {},
   "source": [
    "**Example**: Make all image files in the bucket publicly readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2739acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = { \n",
    "    \"Version\": \"2012-10-17\", \n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": \"*\",\n",
    "            \"Action\": \"s3:GetObject\",\n",
    "            \"Resource\": f\"arn:aws:s3:::{BUCKET_NAME}/images/*\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0044581a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': 'YHK865GHST575M0K', 'HostId': 'dmAEelvLiZxsGIR9mhmiqR8etTOAs6utvDqBDMdO7BZNPB7FgZdcb7iXmhoYQ86JMtcdkCTI/4dY+UuV2BvaXt6bEi6vzXatY5UCFRQALdY=', 'HTTPStatusCode': 204, 'HTTPHeaders': {'x-amz-id-2': 'dmAEelvLiZxsGIR9mhmiqR8etTOAs6utvDqBDMdO7BZNPB7FgZdcb7iXmhoYQ86JMtcdkCTI/4dY+UuV2BvaXt6bEi6vzXatY5UCFRQALdY=', 'x-amz-request-id': 'YHK865GHST575M0K', 'date': 'Wed, 12 Nov 2025 11:23:24 GMT', 'server': 'AmazonS3'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "response = s3_put_bucket_policy(bucket_name=BUCKET_NAME, policy=policy) \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc40df14",
   "metadata": {},
   "source": [
    "Now you can access the image file from the browser by copying its Object Url from AWS Consol\n",
    "\n",
    "![S3 Architecture](assets/img4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f403a0b1",
   "metadata": {},
   "source": [
    "## 8. Enabling Object Versioning\n",
    "Amazon S3 **Versioning** is a powerful feature that allows you to **store multiple versions of the same object** within a bucket.  \n",
    "When versioning is enabled, S3 keeps track of every modification and deletion of your objects, providing a built-in mechanism for **data protection, recovery, and auditability**.\n",
    "By default, an S3 bucket keeps only the latest version of each object. If you upload a new file with the same name, the previous file is **overwritten permanently**.\n",
    "\n",
    "With versioning **enabled**, each time you upload a new version of a file with the same key (name), S3 automatically assigns it a **unique version ID** and preserves the older copies.  \n",
    "You can retrieve or restore any previous version at any time. You can manage versioning at the bucket level using the **`put_bucket_versioning()`** API call.  \n",
    "Below is the Python function to enable or suspend versioning using Boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8521cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_bucket_versioning(bucket_name: str, versioning_config: Dict[str, str]) -> Dict[Any, Any]:\n",
    "    \n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    \n",
    "    # Enable bucket versioning\n",
    "    response = s3_client.put_bucket_versioning(\n",
    "        Bucket=bucket_name,\n",
    "        VersioningConfiguration=versioning_config\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b6a8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "versioning_config = {'Status': 'Enabled'}\n",
    "\n",
    "response = configure_bucket_versioning(bucket_name=BUCKET_NAME, \n",
    "                                       versioning_config=versioning_config)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad1e654",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_file_path_image_v2 = 'data/images/v2/s3_logo.png'\n",
    "object_key_image = 'images/s3_logo.png'\n",
    "\n",
    "upload_file_to_s3(local_file_path_image_v2, BUCKET_NAME, object_key_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ed23f",
   "metadata": {},
   "source": [
    "## 9. Listing Object Versions\n",
    "You can retrieve all versions of objects under a specific prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4730cf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_object_versions(bucket_name: str, prefix_key: str) -> None:\n",
    "    # Create an S3 client\n",
    "    \n",
    "    s3_client = boto3.client(\"s3\")\n",
    "\n",
    "    # List object versions\n",
    "    response = s3_client.list_object_versions(Bucket=bucket_name, Prefix=prefix_key)\n",
    "\n",
    "    # Process the response to get object versions\n",
    "    for version in response.get('Versions', []):\n",
    "        print(\"Object Key:\", version['Key'])\n",
    "        print(\"Object Version Id:\", version['VersionId'])\n",
    "        print(\"Is Latest:\", version['IsLatest'])\n",
    "        print(\"Last Modified:\", version['LastModified'])\n",
    "        print()\n",
    "\n",
    "list_object_versions(bucket_name=BUCKET_NAME, prefix_key='images/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e632646a",
   "metadata": {},
   "source": [
    "## 10. Cleaning Up â€” Deleting Buckets and Objects\n",
    "When you finish testing, you can delete all objects (including versions) and remove the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fbfab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_delete_bucket(bucket_name: str, delete_objects: bool) -> Dict[Any, Any]:\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    if delete_objects:\n",
    "        # List all versions of all objects in the bucket\n",
    "        response = s3_client.list_object_versions(Bucket=bucket_name)\n",
    "        \n",
    "        # Delete all object versions\n",
    "        for version in response.get('Versions', []):\n",
    "            key = version['Key']\n",
    "            version_id = version['VersionId']\n",
    "            s3_client.delete_object(Bucket=bucket_name, Key=key, VersionId=version_id)\n",
    "        \n",
    "        # Delete all delete markers\n",
    "        for delete_marker in response.get('DeleteMarkers', []):\n",
    "            key = delete_marker['Key']\n",
    "            version_id = delete_marker['VersionId']\n",
    "            s3_client.delete_object(Bucket=bucket_name, Key=key, VersionId=version_id)        \n",
    "    \n",
    "    # Delete the bucket\n",
    "    response = s3_client.delete_bucket(\n",
    "        Bucket=bucket_name\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "response = s3_delete_bucket(bucket_name=BUCKET_NAME, delete_objects=True)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
